{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the imports \n",
    "import cv2 \n",
    "import numpy as np \n",
    "import sys\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marwa\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marwa\\anaconda3\\lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marwa\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marwa\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This loads the model \n",
    "if sys.platform == \"linux\":\n",
    "    physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "    if len(physical_devices) > 0:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "\n",
    "model_name = \"movenet_lightning\"  # @param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "\n",
    "if \"tflite\" in model_name:\n",
    "    if \"movenet_lightning_f16\" in model_name:\n",
    "        !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/float16/4?lite-format=tflite\n",
    "        input_size = 192\n",
    "    elif \"movenet_thunder_f16\" in model_name:\n",
    "        !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/float16/4?lite-format=tflite\n",
    "        input_size = 256\n",
    "    elif \"movenet_lightning_int8\" in model_name:\n",
    "        !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/tflite/int8/4?lite-format=tflite\n",
    "        input_size = 192\n",
    "    elif \"movenet_thunder_int8\" in model_name:\n",
    "        !wget -q -O model.tflite https://tfhub.dev/google/lite-model/movenet/singlepose/thunder/tflite/int8/4?lite-format=tflite\n",
    "        input_size = 256\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "    # Initialize the TFLite interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    def movenet(input_image):\n",
    "        \"\"\"Runs detection on an input image.\n",
    "\n",
    "        Args:\n",
    "          input_image: A [1, height, width, 3] tensor represents the input image\n",
    "            pixels. Note that the height/width should already be resized and match the\n",
    "            expected input resolution of the model before passing into this function.\n",
    "\n",
    "        Returns:\n",
    "          A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "          coordinates and scores.\n",
    "        \"\"\"\n",
    "        # TF Lite format expects tensor type of uint8.\n",
    "        input_image = tf.cast(input_image, dtype=tf.uint8)\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        interpreter.set_tensor(input_details[0][\"index\"], input_image.numpy())\n",
    "        # Invoke inference.\n",
    "        interpreter.invoke()\n",
    "        # Get the model prediction.\n",
    "        keypoints_with_scores = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        return keypoints_with_scores\n",
    "\n",
    "\n",
    "else:\n",
    "    if \"movenet_lightning\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/lightning/4\")\n",
    "        input_size = 192\n",
    "    elif \"movenet_thunder\" in model_name:\n",
    "        module = hub.load(\"https://tfhub.dev/google/movenet/singlepose/thunder/4\")\n",
    "        input_size = 256\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "    def movenet(input_image):\n",
    "        \"\"\"Runs detection on an input image.\n",
    "\n",
    "        Args:\n",
    "          input_image: A [1, height, width, 3] tensor represents the input image\n",
    "            pixels. Note that the height/width should already be resized and match the\n",
    "            expected input resolution of the model before passing into this function.\n",
    "\n",
    "        Returns:\n",
    "          A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "          coordinates and scores.\n",
    "        \"\"\"\n",
    "        model = module.signatures[\"serving_default\"]\n",
    "\n",
    "        # SavedModel format expects tensor type of int32.\n",
    "        input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "        # Run model inference.\n",
    "        outputs = model(input_image)\n",
    "        # Output is a [1, 1, 17, 3] tensor.\n",
    "        keypoint_with_scores = outputs[\"output_0\"].numpy()\n",
    "        return keypoint_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keypoint Dictionary \n",
    "# @title Helper functions for visualization\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    \"nose\": 0,\n",
    "    \"left_eye\": 1,\n",
    "    \"right_eye\": 2,\n",
    "    \"left_ear\": 3,\n",
    "    \"right_ear\": 4,\n",
    "    \"left_shoulder\": 5,\n",
    "    \"right_shoulder\": 6,\n",
    "    \"left_elbow\": 7,\n",
    "    \"right_elbow\": 8,\n",
    "    \"left_wrist\": 9,\n",
    "    \"right_wrist\": 10,\n",
    "    \"left_hip\": 11,\n",
    "    \"right_hip\": 12,\n",
    "    \"left_knee\": 13,\n",
    "    \"right_knee\": 14,\n",
    "    \"left_ankle\": 15,\n",
    "    \"right_ankle\": 16,\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): \"m\",\n",
    "    (0, 2): \"c\",\n",
    "    (1, 3): \"m\",\n",
    "    (2, 4): \"c\",\n",
    "    (0, 5): \"m\",\n",
    "    (0, 6): \"c\",\n",
    "    (5, 7): \"m\",\n",
    "    (7, 9): \"m\",\n",
    "    (6, 8): \"c\",\n",
    "    (8, 10): \"c\",\n",
    "    (5, 6): \"y\",\n",
    "    (5, 11): \"m\",\n",
    "    (6, 12): \"c\",\n",
    "    (11, 12): \"y\",\n",
    "    (11, 13): \"m\",\n",
    "    (13, 15): \"m\",\n",
    "    (12, 14): \"c\",\n",
    "    (14, 16): \"c\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is the video processing things, \n",
    "def rotate(image, angle):\n",
    "    \"\"\"\"Rotate image without cropping it\n",
    "    Params:\n",
    "        image - image array in BGR\n",
    "        angle - angle to rotate image\n",
    "    \n",
    "    Returns:\n",
    "        image array in BGR\n",
    "    \"\"\"\n",
    "    h, w, _ = image.shape\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "\n",
    "    rotMat = cv2.getRotationMatrix2D((cX, cY), -angle, 1.0)\n",
    "\n",
    "    cos = np.abs(rotMat[0, 0])\n",
    "    sin = np.abs(rotMat[0, 1])\n",
    "\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    "\n",
    "    rotMat[0, 2] += (nW / 2) - cX\n",
    "    rotMat[1, 2] += (nH / 2) - cY\n",
    "\n",
    "    return cv2.warpAffine(image, rotMat, (nW, nH))\n",
    "\n",
    "\n",
    "# @title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "\n",
    "def init_crop_region(image_height, image_width):\n",
    "    \"\"\"Defines the default crop region.\n",
    "\n",
    "    The function provides the initial crop region (pads the full image from both\n",
    "    sides to make it a square image) when the algorithm cannot reliably determine\n",
    "    the crop region from the previous frame.\n",
    "    \"\"\"\n",
    "    if image_width > image_height:\n",
    "        box_height = image_width / image_height\n",
    "        box_width = 1.0\n",
    "        y_min = (image_height / 2 - image_width / 2) / image_height\n",
    "        x_min = 0.0\n",
    "    else:\n",
    "        box_height = 1.0\n",
    "        box_width = image_height / image_width\n",
    "        y_min = 0.0\n",
    "        x_min = (image_width / 2 - image_height / 2) / image_width\n",
    "\n",
    "    return {\n",
    "        \"y_min\": y_min,\n",
    "        \"x_min\": x_min,\n",
    "        \"y_max\": y_min + box_height,\n",
    "        \"x_max\": x_min + box_width,\n",
    "        \"height\": box_height,\n",
    "        \"width\": box_width,\n",
    "    }\n",
    "\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "    \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "    This function checks whether the model is confident at predicting one of the\n",
    "    shoulders/hips which is required to determine a good crop region.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        keypoints[0, 0, KEYPOINT_DICT[\"left_hip\"], 2] > MIN_CROP_KEYPOINT_SCORE\n",
    "        or keypoints[0, 0, KEYPOINT_DICT[\"right_hip\"], 2] > MIN_CROP_KEYPOINT_SCORE\n",
    "    ) and (\n",
    "        keypoints[0, 0, KEYPOINT_DICT[\"left_shoulder\"], 2] > MIN_CROP_KEYPOINT_SCORE\n",
    "        or keypoints[0, 0, KEYPOINT_DICT[\"right_shoulder\"], 2] > MIN_CROP_KEYPOINT_SCORE\n",
    "    )\n",
    "\n",
    "\n",
    "def determine_torso_and_body_range(keypoints, target_keypoints, center_y, center_x):\n",
    "    \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "    The function returns the maximum distances from the two sets of keypoints:\n",
    "    full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "    used to determine the crop size. See determineCropRegion for more detail.\n",
    "    \"\"\"\n",
    "    torso_joints = [\"left_shoulder\", \"right_shoulder\", \"left_hip\", \"right_hip\"]\n",
    "    max_torso_yrange = 0.0\n",
    "    max_torso_xrange = 0.0\n",
    "    for joint in torso_joints:\n",
    "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "        if dist_y > max_torso_yrange:\n",
    "            max_torso_yrange = dist_y\n",
    "        if dist_x > max_torso_xrange:\n",
    "            max_torso_xrange = dist_x\n",
    "\n",
    "    max_body_yrange = 0.0\n",
    "    max_body_xrange = 0.0\n",
    "    for joint in KEYPOINT_DICT.keys():\n",
    "        if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "            continue\n",
    "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "        if dist_y > max_body_yrange:\n",
    "            max_body_yrange = dist_y\n",
    "\n",
    "        if dist_x > max_body_xrange:\n",
    "            max_body_xrange = dist_x\n",
    "\n",
    "    return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "\n",
    "def determine_crop_region(keypoints, image_height, image_width):\n",
    "    \"\"\"Determines the region to crop the image for the model to run inference on.\n",
    "\n",
    "    The algorithm uses the detected joints from the previous frame to estimate\n",
    "    the square region that encloses the full body of the target person and\n",
    "    centers at the midpoint of two hip joints. The crop size is determined by\n",
    "    the distances between each joints and the center point.\n",
    "    When the model is not confident with the four torso joint predictions, the\n",
    "    function returns a default crop which is the full image padded to square.\n",
    "    \"\"\"\n",
    "    target_keypoints = {}\n",
    "    for joint in KEYPOINT_DICT.keys():\n",
    "        target_keypoints[joint] = [\n",
    "            keypoints[0, 0, KEYPOINT_DICT[joint], 0] * image_height,\n",
    "            keypoints[0, 0, KEYPOINT_DICT[joint], 1] * image_width,\n",
    "        ]\n",
    "\n",
    "    if torso_visible(keypoints):\n",
    "        center_y = (\n",
    "            target_keypoints[\"left_hip\"][0] + target_keypoints[\"right_hip\"][0]\n",
    "        ) / 2\n",
    "        center_x = (\n",
    "            target_keypoints[\"left_hip\"][1] + target_keypoints[\"right_hip\"][1]\n",
    "        ) / 2\n",
    "\n",
    "        (\n",
    "            max_torso_yrange,\n",
    "            max_torso_xrange,\n",
    "            max_body_yrange,\n",
    "            max_body_xrange,\n",
    "        ) = determine_torso_and_body_range(\n",
    "            keypoints, target_keypoints, center_y, center_x\n",
    "        )\n",
    "\n",
    "        crop_length_half = np.amax(\n",
    "            [\n",
    "                max_torso_xrange * 1.9,\n",
    "                max_torso_yrange * 1.9,\n",
    "                max_body_yrange * 1.2,\n",
    "                max_body_xrange * 1.2,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        tmp = np.array(\n",
    "            [center_x, image_width - center_x, center_y, image_height - center_y]\n",
    "        )\n",
    "        crop_length_half = np.amin([crop_length_half, np.amax(tmp)])\n",
    "\n",
    "        crop_corner = [center_y - crop_length_half, center_x - crop_length_half]\n",
    "\n",
    "        if crop_length_half > max(image_width, image_height) / 2:\n",
    "            return init_crop_region(image_height, image_width)\n",
    "        else:\n",
    "            crop_length = crop_length_half * 2\n",
    "            return {\n",
    "                \"y_min\": crop_corner[0] / image_height,\n",
    "                \"x_min\": crop_corner[1] / image_width,\n",
    "                \"y_max\": (crop_corner[0] + crop_length) / image_height,\n",
    "                \"x_max\": (crop_corner[1] + crop_length) / image_width,\n",
    "                \"height\": (crop_corner[0] + crop_length) / image_height\n",
    "                - crop_corner[0] / image_height,\n",
    "                \"width\": (crop_corner[1] + crop_length) / image_width\n",
    "                - crop_corner[1] / image_width,\n",
    "            }\n",
    "    else:\n",
    "        return init_crop_region(image_height, image_width)\n",
    "\n",
    "\n",
    "def crop_and_resize(image, crop_region, crop_size):\n",
    "    \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "    boxes = [\n",
    "        [\n",
    "            crop_region[\"y_min\"],\n",
    "            crop_region[\"x_min\"],\n",
    "            crop_region[\"y_max\"],\n",
    "            crop_region[\"x_max\"],\n",
    "        ]\n",
    "    ]\n",
    "    output_image = tf.image.crop_and_resize(\n",
    "        image, box_indices=[0], boxes=boxes, crop_size=crop_size\n",
    "    )\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def run_inference(movenet, image, crop_region, crop_size):\n",
    "    \"\"\"Runs model inferece on the cropped region.\n",
    "\n",
    "    The function runs the model inference on the cropped region and updates the\n",
    "    model output to the original image coordinate system.\n",
    "    \"\"\"\n",
    "    image_height, image_width, _ = image.shape\n",
    "    input_image = crop_and_resize(\n",
    "        tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size\n",
    "    )\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    # Update the coordinates.\n",
    "    for idx in range(17):\n",
    "        keypoints_with_scores[0, 0, idx, 0] = (\n",
    "            crop_region[\"y_min\"] * image_height\n",
    "            + crop_region[\"height\"] * image_height * keypoints_with_scores[0, 0, idx, 0]\n",
    "        ) / image_height\n",
    "        keypoints_with_scores[0, 0, idx, 1] = (\n",
    "            crop_region[\"x_min\"] * image_width\n",
    "            + crop_region[\"width\"] * image_width * keypoints_with_scores[0, 0, idx, 1]\n",
    "        ) / image_width\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function takes the video and splits into the video we would like\n",
    "def video_processor(file_path: str, file_name: str):\n",
    "    '''\n",
    "    '''\n",
    "    video_path = file_path \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    #Now we need to extract the size \n",
    "    keypoints = []\n",
    "    i = 0 \n",
    "    ret, frame = cap.read() \n",
    "    frame = rotate(frame, 90)\n",
    "\n",
    "    image_height, image_width, _ = frame.shape \n",
    "    crop_region = init_crop_region(image_height, image_width)\n",
    "    while cap.isOpened():\n",
    "        print(\"frame: \", i)\n",
    "\n",
    "        keypoints_with_scores = run_inference(\n",
    "            movenet, frame, crop_region, crop_size=[input_size, input_size]\n",
    "        )\n",
    "\n",
    "        keypoints.append(keypoints_with_scores)\n",
    "\n",
    "\n",
    "        crop_region = determine_crop_region (\n",
    "            keypoints_with_scores, image_height, image_width\n",
    "        )\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame = rotate(frame, 90)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    cap.release()\n",
    "    np.save(file_name, np.array(keypoints))\n",
    "    return np.load(file_name)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame:  0\n",
      "frame:  1\n",
      "frame:  2\n",
      "frame:  3\n",
      "frame:  4\n",
      "frame:  5\n",
      "frame:  6\n",
      "frame:  7\n",
      "frame:  8\n",
      "frame:  9\n",
      "frame:  10\n",
      "frame:  11\n",
      "frame:  12\n",
      "frame:  13\n",
      "frame:  14\n",
      "frame:  15\n",
      "frame:  16\n",
      "frame:  17\n",
      "frame:  18\n",
      "frame:  19\n",
      "frame:  20\n",
      "frame:  21\n",
      "frame:  22\n",
      "frame:  23\n",
      "frame:  24\n",
      "frame:  25\n",
      "frame:  26\n",
      "frame:  27\n",
      "frame:  28\n",
      "frame:  29\n",
      "frame:  30\n",
      "frame:  31\n",
      "frame:  32\n",
      "frame:  33\n",
      "frame:  34\n",
      "frame:  35\n",
      "frame:  36\n",
      "frame:  37\n",
      "frame:  38\n",
      "frame:  39\n",
      "frame:  40\n",
      "frame:  41\n",
      "frame:  42\n",
      "frame:  43\n",
      "frame:  44\n",
      "frame:  45\n",
      "frame:  46\n",
      "frame:  47\n",
      "frame:  48\n",
      "frame:  49\n",
      "frame:  50\n",
      "frame:  51\n",
      "frame:  52\n",
      "frame:  53\n",
      "frame:  54\n",
      "frame:  55\n",
      "frame:  56\n",
      "frame:  57\n",
      "frame:  58\n",
      "frame:  59\n",
      "frame:  60\n",
      "frame:  61\n",
      "frame:  62\n",
      "frame:  63\n",
      "frame:  64\n",
      "frame:  65\n",
      "frame:  66\n",
      "frame:  67\n",
      "frame:  68\n",
      "frame:  69\n",
      "frame:  70\n",
      "frame:  71\n",
      "frame:  72\n",
      "frame:  73\n",
      "frame:  74\n",
      "frame:  75\n",
      "frame:  76\n",
      "frame:  77\n",
      "frame:  78\n",
      "frame:  79\n",
      "frame:  80\n",
      "frame:  81\n",
      "frame:  82\n",
      "frame:  83\n",
      "frame:  84\n",
      "frame:  85\n",
      "frame:  86\n",
      "frame:  87\n",
      "frame:  88\n",
      "frame:  89\n",
      "frame:  90\n",
      "frame:  91\n",
      "frame:  92\n",
      "frame:  93\n",
      "frame:  94\n",
      "frame:  95\n",
      "frame:  96\n",
      "frame:  97\n",
      "frame:  98\n",
      "frame:  99\n",
      "frame:  100\n",
      "frame:  101\n",
      "frame:  102\n",
      "frame:  103\n",
      "frame:  104\n",
      "frame:  105\n",
      "frame:  106\n",
      "frame:  107\n",
      "frame:  108\n",
      "frame:  109\n",
      "frame:  110\n",
      "frame:  111\n",
      "frame:  112\n",
      "frame:  113\n",
      "frame:  114\n",
      "frame:  115\n",
      "frame:  116\n",
      "frame:  117\n",
      "frame:  118\n",
      "frame:  119\n",
      "frame:  120\n",
      "frame:  121\n",
      "frame:  122\n",
      "frame:  123\n",
      "frame:  124\n",
      "frame:  125\n",
      "frame:  126\n",
      "frame:  127\n",
      "frame:  128\n",
      "frame:  129\n",
      "frame:  130\n",
      "frame:  131\n",
      "frame:  132\n",
      "frame:  133\n",
      "frame:  134\n",
      "frame:  135\n",
      "frame:  136\n",
      "frame:  137\n",
      "frame:  138\n",
      "frame:  139\n",
      "frame:  140\n",
      "frame:  141\n",
      "frame:  142\n",
      "frame:  143\n",
      "frame:  144\n",
      "frame:  145\n",
      "frame:  146\n",
      "frame:  147\n",
      "frame:  148\n",
      "frame:  149\n",
      "frame:  150\n",
      "frame:  151\n",
      "frame:  152\n",
      "frame:  153\n",
      "frame:  154\n",
      "frame:  155\n",
      "frame:  156\n",
      "frame:  157\n",
      "frame:  158\n",
      "frame:  159\n",
      "frame:  160\n",
      "frame:  161\n",
      "frame:  162\n",
      "frame:  163\n",
      "frame:  164\n",
      "frame:  165\n",
      "frame:  166\n",
      "frame:  167\n",
      "frame:  168\n",
      "frame:  169\n",
      "frame:  170\n",
      "frame:  171\n",
      "frame:  172\n",
      "frame:  173\n",
      "frame:  174\n",
      "frame:  175\n",
      "frame:  176\n",
      "frame:  177\n",
      "frame:  178\n",
      "frame:  179\n",
      "frame:  180\n",
      "frame:  181\n",
      "frame:  182\n",
      "frame:  183\n",
      "frame:  184\n",
      "frame:  185\n",
      "frame:  186\n",
      "frame:  187\n",
      "frame:  188\n",
      "frame:  189\n",
      "frame:  190\n",
      "frame:  191\n",
      "frame:  192\n",
      "frame:  193\n",
      "frame:  194\n",
      "frame:  195\n",
      "frame:  196\n",
      "frame:  197\n",
      "frame:  198\n",
      "frame:  199\n",
      "frame:  200\n",
      "frame:  201\n",
      "frame:  202\n",
      "frame:  203\n",
      "frame:  204\n",
      "frame:  205\n",
      "frame:  206\n",
      "frame:  207\n",
      "frame:  208\n",
      "frame:  209\n",
      "frame:  210\n",
      "frame:  211\n",
      "frame:  212\n",
      "frame:  213\n",
      "frame:  214\n",
      "frame:  215\n",
      "frame:  216\n",
      "frame:  217\n",
      "frame:  218\n",
      "frame:  219\n",
      "frame:  220\n",
      "frame:  221\n",
      "frame:  222\n",
      "frame:  223\n",
      "frame:  224\n",
      "frame:  225\n",
      "frame:  226\n",
      "frame:  227\n",
      "frame:  228\n",
      "frame:  229\n",
      "frame:  230\n",
      "frame:  231\n",
      "frame:  232\n",
      "frame:  233\n",
      "frame:  234\n",
      "frame:  235\n",
      "frame:  236\n",
      "frame:  237\n",
      "frame:  238\n",
      "frame:  239\n",
      "frame:  240\n",
      "frame:  241\n",
      "frame:  242\n",
      "frame:  243\n",
      "frame:  244\n",
      "frame:  245\n",
      "frame:  246\n",
      "frame:  247\n",
      "frame:  248\n",
      "frame:  249\n",
      "frame:  250\n",
      "frame:  251\n",
      "frame:  252\n",
      "frame:  253\n",
      "frame:  254\n",
      "frame:  255\n",
      "frame:  256\n",
      "frame:  257\n",
      "frame:  258\n",
      "frame:  259\n",
      "frame:  260\n",
      "frame:  261\n",
      "frame:  262\n",
      "frame:  263\n",
      "frame:  264\n",
      "frame:  265\n",
      "frame:  266\n",
      "frame:  267\n",
      "frame:  268\n",
      "frame:  269\n",
      "frame:  270\n",
      "frame:  271\n",
      "frame:  272\n",
      "frame:  273\n",
      "frame:  274\n",
      "frame:  275\n",
      "frame:  276\n",
      "frame:  277\n",
      "frame:  278\n",
      "frame:  279\n",
      "frame:  280\n",
      "frame:  281\n",
      "frame:  282\n",
      "frame:  283\n",
      "frame:  284\n",
      "frame:  285\n",
      "frame:  286\n",
      "frame:  287\n",
      "frame:  288\n",
      "frame:  289\n",
      "frame:  290\n",
      "frame:  291\n",
      "frame:  292\n",
      "frame:  293\n",
      "frame:  294\n",
      "frame:  295\n",
      "frame:  296\n",
      "frame:  297\n",
      "frame:  298\n",
      "frame:  299\n",
      "frame:  300\n",
      "frame:  301\n",
      "frame:  302\n",
      "frame:  303\n",
      "frame:  304\n",
      "frame:  305\n",
      "frame:  306\n",
      "frame:  307\n",
      "frame:  308\n",
      "frame:  309\n",
      "frame:  310\n",
      "frame:  311\n",
      "frame:  312\n",
      "frame:  313\n",
      "frame:  314\n",
      "frame:  315\n",
      "frame:  316\n",
      "frame:  317\n",
      "frame:  318\n",
      "frame:  319\n",
      "frame:  320\n",
      "frame:  321\n",
      "frame:  322\n",
      "frame:  323\n",
      "frame:  324\n",
      "frame:  325\n",
      "frame:  326\n",
      "frame:  327\n",
      "frame:  328\n",
      "frame:  329\n",
      "frame:  330\n",
      "frame:  331\n",
      "frame:  332\n",
      "frame:  333\n",
      "frame:  334\n",
      "frame:  335\n",
      "frame:  336\n",
      "frame:  337\n",
      "frame:  338\n",
      "frame:  339\n",
      "frame:  340\n",
      "frame:  341\n",
      "frame:  342\n",
      "frame:  343\n",
      "frame:  344\n",
      "frame:  345\n",
      "frame:  346\n",
      "frame:  347\n",
      "frame:  348\n",
      "frame:  349\n",
      "frame:  350\n",
      "frame:  351\n",
      "frame:  352\n",
      "frame:  353\n",
      "frame:  354\n",
      "frame:  355\n",
      "frame:  356\n",
      "frame:  357\n",
      "frame:  358\n",
      "frame:  359\n",
      "frame:  360\n",
      "frame:  361\n",
      "frame:  362\n",
      "frame:  363\n",
      "frame:  364\n",
      "frame:  365\n",
      "frame:  366\n",
      "frame:  367\n",
      "frame:  368\n",
      "frame:  369\n",
      "frame:  370\n",
      "frame:  371\n",
      "frame:  372\n",
      "frame:  373\n",
      "frame:  374\n",
      "frame:  375\n",
      "frame:  376\n",
      "frame:  377\n",
      "frame:  378\n",
      "frame:  379\n",
      "frame:  380\n",
      "frame:  381\n",
      "frame:  382\n",
      "frame:  383\n",
      "frame:  384\n",
      "frame:  385\n",
      "frame:  386\n",
      "frame:  387\n",
      "frame:  388\n",
      "frame:  389\n",
      "frame:  390\n",
      "frame:  391\n",
      "frame:  392\n",
      "frame:  393\n",
      "frame:  394\n",
      "frame:  395\n",
      "frame:  396\n",
      "frame:  397\n",
      "frame:  398\n",
      "frame:  399\n",
      "frame:  400\n",
      "frame:  401\n",
      "frame:  402\n",
      "frame:  403\n",
      "frame:  404\n",
      "frame:  405\n",
      "frame:  406\n",
      "frame:  407\n",
      "frame:  408\n",
      "frame:  409\n",
      "frame:  410\n",
      "frame:  411\n",
      "frame:  412\n",
      "frame:  413\n",
      "frame:  414\n",
      "frame:  415\n",
      "frame:  416\n",
      "frame:  417\n",
      "frame:  418\n",
      "frame:  419\n",
      "frame:  420\n",
      "frame:  421\n",
      "frame:  422\n",
      "frame:  423\n",
      "frame:  424\n",
      "frame:  425\n",
      "frame:  426\n",
      "frame:  427\n",
      "frame:  428\n",
      "frame:  429\n",
      "frame:  430\n",
      "frame:  431\n",
      "frame:  432\n",
      "frame:  433\n",
      "frame:  434\n",
      "frame:  435\n",
      "frame:  436\n",
      "frame:  437\n",
      "frame:  438\n",
      "frame:  439\n",
      "frame:  440\n",
      "frame:  441\n",
      "frame:  442\n",
      "frame:  443\n",
      "frame:  444\n",
      "frame:  445\n",
      "frame:  446\n",
      "frame:  447\n",
      "frame:  448\n",
      "frame:  449\n",
      "frame:  450\n",
      "frame:  451\n",
      "frame:  452\n",
      "frame:  453\n",
      "frame:  454\n",
      "frame:  455\n",
      "frame:  456\n",
      "frame:  457\n",
      "frame:  458\n",
      "frame:  459\n",
      "frame:  460\n",
      "frame:  461\n",
      "frame:  462\n",
      "frame:  463\n",
      "frame:  464\n",
      "frame:  465\n",
      "frame:  466\n",
      "frame:  467\n",
      "frame:  468\n",
      "frame:  469\n",
      "frame:  470\n",
      "frame:  471\n",
      "frame:  472\n",
      "frame:  473\n",
      "frame:  474\n",
      "frame:  475\n",
      "frame:  476\n",
      "frame:  477\n",
      "frame:  478\n",
      "frame:  479\n",
      "frame:  480\n",
      "frame:  481\n",
      "frame:  482\n",
      "frame:  483\n",
      "frame:  484\n",
      "frame:  485\n",
      "frame:  486\n",
      "frame:  487\n",
      "frame:  488\n",
      "frame:  489\n",
      "frame:  490\n",
      "frame:  491\n",
      "frame:  492\n",
      "frame:  493\n",
      "frame:  494\n",
      "frame:  495\n",
      "frame:  496\n",
      "frame:  497\n",
      "frame:  498\n",
      "frame:  499\n",
      "frame:  500\n",
      "frame:  501\n",
      "frame:  502\n",
      "frame:  503\n",
      "frame:  504\n",
      "frame:  505\n",
      "frame:  506\n",
      "frame:  507\n",
      "frame:  508\n",
      "frame:  509\n",
      "frame:  510\n",
      "frame:  511\n",
      "frame:  512\n",
      "frame:  513\n",
      "frame:  514\n",
      "frame:  515\n",
      "frame:  516\n",
      "frame:  517\n",
      "frame:  518\n",
      "frame:  519\n",
      "frame:  520\n",
      "frame:  521\n",
      "frame:  522\n",
      "frame:  523\n",
      "frame:  524\n",
      "frame:  525\n",
      "frame:  526\n",
      "frame:  527\n",
      "frame:  528\n",
      "frame:  529\n",
      "frame:  530\n",
      "frame:  531\n",
      "frame:  532\n",
      "frame:  533\n",
      "frame:  534\n",
      "frame:  535\n",
      "frame:  536\n",
      "frame:  537\n",
      "frame:  538\n",
      "frame:  539\n",
      "frame:  540\n",
      "frame:  541\n",
      "frame:  542\n",
      "frame:  543\n",
      "frame:  544\n",
      "frame:  545\n",
      "frame:  546\n",
      "frame:  547\n",
      "frame:  548\n",
      "frame:  549\n",
      "frame:  550\n",
      "frame:  551\n",
      "frame:  552\n",
      "frame:  553\n",
      "frame:  554\n",
      "frame:  555\n",
      "frame:  556\n",
      "frame:  557\n",
      "frame:  558\n",
      "frame:  559\n",
      "frame:  560\n",
      "frame:  561\n",
      "frame:  562\n",
      "frame:  563\n",
      "frame:  564\n",
      "frame:  565\n",
      "frame:  566\n",
      "frame:  567\n",
      "frame:  568\n",
      "frame:  569\n",
      "frame:  570\n",
      "frame:  571\n",
      "frame:  572\n",
      "frame:  573\n",
      "frame:  574\n",
      "frame:  575\n",
      "frame:  576\n",
      "frame:  577\n",
      "frame:  578\n",
      "frame:  579\n",
      "frame:  580\n",
      "frame:  581\n",
      "frame:  582\n",
      "frame:  583\n",
      "frame:  584\n",
      "frame:  585\n",
      "frame:  586\n",
      "frame:  587\n",
      "frame:  588\n",
      "frame:  589\n",
      "frame:  590\n",
      "frame:  591\n",
      "frame:  592\n",
      "frame:  593\n",
      "frame:  594\n",
      "frame:  595\n",
      "frame:  596\n",
      "frame:  597\n",
      "frame:  598\n",
      "frame:  599\n",
      "frame:  600\n",
      "frame:  601\n",
      "frame:  602\n",
      "frame:  603\n",
      "frame:  604\n",
      "frame:  605\n",
      "frame:  606\n",
      "frame:  607\n",
      "frame:  608\n",
      "frame:  609\n",
      "frame:  610\n",
      "frame:  611\n",
      "frame:  612\n",
      "frame:  613\n",
      "frame:  614\n",
      "frame:  615\n",
      "frame:  616\n",
      "frame:  617\n",
      "frame:  618\n",
      "frame:  619\n",
      "frame:  620\n",
      "frame:  621\n",
      "frame:  622\n",
      "frame:  623\n",
      "frame:  624\n",
      "frame:  625\n",
      "frame:  626\n",
      "frame:  627\n",
      "frame:  628\n",
      "frame:  629\n",
      "frame:  630\n",
      "frame:  631\n",
      "frame:  632\n",
      "frame:  633\n",
      "frame:  634\n",
      "frame:  635\n",
      "frame:  636\n",
      "frame:  637\n",
      "frame:  638\n",
      "frame:  639\n",
      "frame:  640\n",
      "frame:  641\n",
      "frame:  642\n",
      "frame:  643\n",
      "frame:  644\n",
      "frame:  645\n",
      "frame:  646\n",
      "frame:  647\n",
      "frame:  648\n",
      "frame:  649\n",
      "frame:  650\n",
      "frame:  651\n",
      "frame:  652\n",
      "frame:  653\n",
      "frame:  654\n",
      "frame:  655\n",
      "frame:  656\n",
      "frame:  657\n",
      "frame:  658\n",
      "frame:  659\n",
      "frame:  660\n",
      "frame:  661\n",
      "frame:  662\n",
      "frame:  663\n",
      "frame:  664\n",
      "frame:  665\n",
      "frame:  666\n",
      "frame:  667\n",
      "frame:  668\n",
      "frame:  669\n",
      "frame:  670\n",
      "frame:  671\n",
      "frame:  672\n",
      "frame:  673\n",
      "frame:  674\n",
      "frame:  675\n",
      "frame:  676\n",
      "frame:  677\n",
      "frame:  678\n",
      "frame:  679\n",
      "frame:  680\n",
      "frame:  681\n",
      "frame:  682\n",
      "frame:  683\n",
      "frame:  684\n",
      "frame:  685\n",
      "frame:  686\n",
      "frame:  687\n",
      "frame:  688\n",
      "frame:  689\n",
      "frame:  690\n",
      "frame:  691\n",
      "frame:  692\n",
      "frame:  693\n",
      "frame:  694\n",
      "frame:  695\n",
      "frame:  696\n",
      "frame:  697\n",
      "frame:  698\n",
      "frame:  699\n",
      "frame:  700\n",
      "frame:  701\n",
      "frame:  702\n",
      "frame:  703\n",
      "frame:  704\n",
      "frame:  705\n",
      "frame:  706\n",
      "frame:  707\n",
      "frame:  708\n",
      "frame:  709\n",
      "frame:  710\n",
      "frame:  711\n",
      "frame:  712\n",
      "frame:  713\n",
      "frame:  714\n",
      "frame:  715\n",
      "frame:  716\n",
      "frame:  717\n",
      "frame:  718\n",
      "frame:  719\n",
      "frame:  720\n",
      "frame:  721\n",
      "frame:  722\n",
      "frame:  723\n",
      "frame:  724\n",
      "frame:  725\n",
      "frame:  726\n",
      "frame:  727\n",
      "frame:  728\n",
      "frame:  729\n",
      "frame:  730\n",
      "frame:  731\n",
      "frame:  732\n",
      "frame:  733\n",
      "frame:  734\n",
      "frame:  735\n",
      "frame:  736\n",
      "frame:  737\n",
      "frame:  738\n",
      "frame:  739\n",
      "frame:  740\n",
      "frame:  741\n",
      "frame:  742\n",
      "frame:  743\n",
      "frame:  744\n",
      "frame:  745\n",
      "frame:  746\n",
      "frame:  747\n",
      "frame:  748\n",
      "frame:  749\n",
      "frame:  750\n",
      "frame:  751\n",
      "frame:  752\n",
      "frame:  753\n",
      "frame:  754\n",
      "frame:  755\n",
      "frame:  756\n",
      "frame:  757\n",
      "frame:  758\n",
      "frame:  759\n",
      "frame:  760\n",
      "frame:  761\n",
      "frame:  762\n",
      "frame:  763\n",
      "frame:  764\n",
      "frame:  765\n",
      "frame:  766\n",
      "frame:  767\n",
      "frame:  768\n",
      "frame:  769\n",
      "frame:  770\n",
      "frame:  771\n",
      "frame:  772\n",
      "frame:  773\n",
      "frame:  774\n",
      "frame:  775\n",
      "frame:  776\n",
      "frame:  777\n",
      "frame:  778\n",
      "frame:  779\n",
      "frame:  780\n",
      "frame:  781\n",
      "frame:  782\n",
      "frame:  783\n",
      "frame:  784\n",
      "frame:  785\n",
      "frame:  786\n",
      "frame:  787\n",
      "frame:  788\n",
      "frame:  789\n",
      "frame:  790\n",
      "frame:  791\n",
      "frame:  792\n",
      "frame:  793\n",
      "frame:  794\n",
      "frame:  795\n",
      "frame:  796\n",
      "frame:  797\n",
      "frame:  798\n",
      "frame:  799\n",
      "frame:  800\n",
      "frame:  801\n",
      "frame:  802\n",
      "frame:  803\n",
      "frame:  804\n",
      "frame:  805\n",
      "frame:  806\n",
      "frame:  807\n",
      "frame:  808\n",
      "frame:  809\n",
      "frame:  810\n",
      "frame:  811\n",
      "frame:  812\n",
      "frame:  813\n",
      "frame:  814\n",
      "frame:  815\n",
      "frame:  816\n",
      "frame:  817\n",
      "frame:  818\n",
      "frame:  819\n",
      "frame:  820\n",
      "frame:  821\n",
      "frame:  822\n",
      "frame:  823\n",
      "frame:  824\n",
      "frame:  825\n",
      "frame:  826\n",
      "frame:  827\n",
      "frame:  828\n",
      "frame:  829\n",
      "frame:  830\n",
      "frame:  831\n",
      "frame:  832\n",
      "frame:  833\n",
      "frame:  834\n",
      "frame:  835\n",
      "frame:  836\n",
      "frame:  837\n",
      "frame:  838\n",
      "frame:  839\n",
      "frame:  840\n",
      "frame:  841\n",
      "frame:  842\n",
      "frame:  843\n",
      "frame:  844\n",
      "frame:  845\n",
      "frame:  846\n",
      "frame:  847\n",
      "frame:  848\n",
      "frame:  849\n",
      "frame:  850\n",
      "frame:  851\n",
      "frame:  852\n",
      "frame:  853\n",
      "frame:  854\n",
      "frame:  855\n",
      "frame:  856\n",
      "frame:  857\n",
      "frame:  858\n",
      "frame:  859\n",
      "frame:  860\n",
      "frame:  861\n",
      "frame:  862\n",
      "frame:  863\n",
      "frame:  864\n",
      "frame:  865\n",
      "frame:  866\n",
      "frame:  867\n",
      "frame:  868\n",
      "frame:  869\n",
      "frame:  870\n",
      "frame:  871\n",
      "frame:  872\n",
      "frame:  873\n",
      "frame:  874\n",
      "frame:  875\n",
      "frame:  876\n",
      "frame:  877\n",
      "frame:  878\n",
      "frame:  879\n",
      "frame:  880\n",
      "frame:  881\n",
      "frame:  882\n",
      "frame:  883\n",
      "frame:  884\n",
      "frame:  885\n",
      "frame:  886\n",
      "frame:  887\n",
      "frame:  888\n",
      "frame:  889\n",
      "frame:  890\n",
      "frame:  891\n",
      "frame:  892\n",
      "frame:  893\n",
      "frame:  894\n",
      "frame:  895\n",
      "frame:  896\n",
      "frame:  897\n",
      "frame:  898\n",
      "frame:  899\n",
      "frame:  900\n",
      "frame:  901\n",
      "frame:  902\n",
      "frame:  903\n",
      "frame:  904\n",
      "frame:  905\n",
      "frame:  906\n",
      "frame:  907\n",
      "frame:  908\n",
      "frame:  909\n",
      "frame:  910\n",
      "frame:  911\n",
      "frame:  912\n",
      "frame:  913\n",
      "frame:  914\n",
      "frame:  915\n",
      "frame:  916\n",
      "frame:  917\n",
      "frame:  918\n",
      "frame:  919\n",
      "frame:  920\n",
      "frame:  921\n",
      "frame:  922\n",
      "frame:  923\n",
      "frame:  924\n",
      "frame:  925\n",
      "frame:  926\n",
      "frame:  927\n",
      "frame:  928\n",
      "frame:  929\n",
      "frame:  930\n",
      "frame:  931\n",
      "frame:  932\n",
      "frame:  933\n",
      "frame:  934\n",
      "frame:  935\n",
      "frame:  936\n",
      "frame:  937\n",
      "frame:  938\n",
      "frame:  939\n",
      "frame:  940\n",
      "frame:  941\n",
      "frame:  942\n",
      "frame:  943\n",
      "frame:  944\n",
      "frame:  945\n",
      "frame:  946\n",
      "frame:  947\n",
      "frame:  948\n",
      "frame:  949\n",
      "frame:  950\n",
      "frame:  951\n",
      "(952, 1, 1, 17, 3)\n"
     ]
    }
   ],
   "source": [
    "video_frames = video_processor(\"data/video/id0_jab_1.mp4\", \"id0_labels.npy\")\n",
    "print(video_frames.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "This syntax can be used to open the file \n",
    "\n",
    "model = create_model(....)\n",
    "\n",
    "with open(\"file_name.mp4\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
