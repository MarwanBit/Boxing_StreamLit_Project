{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement opencv (from versions: none)\n",
      "ERROR: No matching distribution found for opencv\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv\n",
    "\n",
    "from movenet import get_pose_net\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch \n",
    "import cv2\n",
    "import model_factory\n",
    "cuda = False\n",
    "device = torch.device(\"cpu\")\n",
    "input_size = 192\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_factory.load_model(\"movenet_lightning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's try to do prediction\n",
    "video_path = \"data/video/id0_jab_1.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "#load this \n",
    "first_frame = cap.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1920, 1080])\n",
      "torch.Size([1, 1, 1, 1920, 1080])\n",
      "torch.Size([1, 1, 1, 192, 192])\n",
      "torch.Size([1, 192, 192, 3])\n",
      "torch.Size([1, 1, 17, 3])\n"
     ]
    }
   ],
   "source": [
    "#Let's evaluate the model\n",
    "first_frame = first_frame[1]\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "first_frame = torch.Tensor(first_frame)\n",
    "# print(first_frame)\n",
    "# print(first_frame.shape)\n",
    "\n",
    "channel_1 = torch.empty((1, 1920, 1080))\n",
    "channel_2 = torch.empty((1, 1920, 1080))\n",
    "channel_3 = torch.empty((1, 1920, 1080))\n",
    "\n",
    "channel_1 = first_frame[:,:,0]\n",
    "channel_2 = first_frame[:,:,1]\n",
    "channel_3 = first_frame[:,:,2]\n",
    "print(channel_1.shape)\n",
    "channel_1 = channel_1[None, None, None, :,:]\n",
    "channel_2 = channel_2[None, None, None, :,:]\n",
    "channel_3 = channel_3[None, None, None, :,:]\n",
    "assert channel_1.shape == (1,1,1, 1920, 1080)\n",
    "print(channel_1.shape)\n",
    "\n",
    "channel_1 = F.interpolate(channel_1,size = (1,192,192))\n",
    "channel_2 = F.interpolate(channel_2, size = (1,192,192))\n",
    "channel_3 = F.interpolate(channel_3, size=(1,192,192))\n",
    "print(channel_1.shape)\n",
    "\n",
    "new_output = torch.empty((1,192,192,3))\n",
    "new_output[0,:,:,0] = channel_1[0,0,0,:,:]\n",
    "new_output[0,:,:,1] = channel_2[0,0,0,:,:]\n",
    "new_output[0,:,:,2] = channel_3[0,0,0,:,:]\n",
    "\n",
    "print(new_output.shape) \n",
    "\n",
    "new_output = new_output.to(device)\n",
    "output = model(new_output)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
